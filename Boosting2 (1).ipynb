{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1e812f-6eb0-4566-aec4-8f354fdc0407",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Gradient Boosting Regression is an ensemble learning technique used for regression tasks. It builds the model sequentially by combining multiple weak learners, typically decision trees, to form a strong learner. The key idea is to minimize the loss function by adding new models that correct the errors of the previous models. Gradient Boosting uses gradient descent to optimize the loss function, hence the name \"Gradient Boosting.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc44ad7-7439-4dc7-9353-8401e629ca49",
   "metadata": {},
   "source": [
    "Q2. Implementing Gradient Boosting Regression from Scratch\n",
    "Here is a simple implementation of a gradient boosting algorithm for regression using Python and NumPy:\n",
    "\n",
    "Step-by-Step Implementation:\n",
    "Initialize: Start with a constant model.\n",
    "Iterate: For a specified number of iterations:\n",
    "Compute the negative gradient (residuals).\n",
    "Fit a weak learner (e.g., a decision tree) to the residuals.\n",
    "Update the model by adding the new learner scaled by a learning rate.\n",
    "Predict: Combine the predictions of all learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4bcc59-899a-41ee-b631-680032e7821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.16\n",
      "R-squared: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple regression dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100)[:, np.newaxis]\n",
    "y = 2 * X.flatten() + 1 + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Gradient Boosting Regressor from scratch\n",
    "class GradientBoostingRegressorScratch:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initial prediction is the mean of y\n",
    "        self.initial_pred = np.mean(y)\n",
    "        y_pred = np.full(y.shape, self.initial_pred)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            update = self.learning_rate * tree.predict(X)\n",
    "            y_pred += update\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.initial_pred)\n",
    "        for tree in self.models:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Instantiate and train the model\n",
    "gbr = GradientBoostingRegressorScratch(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gbr.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086b772-a599-4b14-9f0b-7866f44a77a2",
   "metadata": {},
   "source": [
    "Q3. Experimenting with Hyperparameters\n",
    "To optimize the performance of the model, we can experiment with different hyperparameters like learning rate, number of trees (estimators), and tree depth. We can use Grid Search to find the best combination of these hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6acba0cf-291c-4d34-a447-eb8aee539409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50}\n",
      "Best score: -15.23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Wrap our custom Gradient Boosting Regressor in a scikit-learn compatible interface\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class GradientBoostingRegressorScratchWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.model = GradientBoostingRegressorScratch(\n",
    "            n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "gbr_wrapper = GradientBoostingRegressorScratchWrapper()\n",
    "grid_search = GridSearchCV(gbr_wrapper, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Output the best parameters and the best score\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best score: {grid_search.best_score_:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b809616-5624-440d-9ebe-dc76c9df5b22",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing. In the context of Gradient Boosting, it is typically a shallow decision tree with limited depth. The weak learner's job is to correct the errors made by the previous learners in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702fc8b-b2c6-498b-9e99-b9e95092a79c",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to build a strong predictive model by combining many weak learners sequentially. Each new learner is trained to correct the errors made by the combined ensemble of all previous learners. By focusing on the residuals (errors) of the previous learners, Gradient Boosting effectively reduces bias and improves the overall model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9ea70-d4e3-483f-9498-33b4c358ddda",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners through the following steps:\n",
    "\n",
    "Initialization: Start with a constant prediction (e.g., the mean of the target values).\n",
    "Additive Modeling: Iteratively add new models (weak learners) that minimize the loss function.\n",
    "Residual Calculation: For each iteration, compute the residuals (difference between actual and predicted values).\n",
    "Fit New Model: Train a new weak learner on the residuals.\n",
    "Update Model: Update the ensemble by adding the new modelâ€™s predictions, scaled by a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57cddec-d955-4ba7-8d1c-4cfa75238358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
